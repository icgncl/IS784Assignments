{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "e216642_Assignment1_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAI8gIO4RBBC"
      },
      "source": [
        "# IS 784 Assignment1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OcmaWOHdN2Y"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRgLvny9XTuf"
      },
      "source": [
        "\n",
        "\n",
        "In the first part of the assignment, I have filled necessary parts, and obtained a result. I have used \"basic_english\" tokenizer instead of \"spacy\" tokenizer because \"spacy\" tokenizer was taking too much time when it is compared to \"basic_english\" tokenizer\n",
        "In the second part, I have tested it with\n",
        "only stemming, only with lemmatization and lemmatization-removing stop words.\n",
        "\n",
        "\n",
        "*   **Only stemming :** I have used SnowballStemmer from nltk and obtained 62.62% accuracy with it.\n",
        "*   **Only lemmatization :** I have used WordNetLemmatizer again from nltk and obtained 62.82% accuracy.\n",
        "*   **Lemmatization and Removing Stop Words :** In this part, I have removed stop words like 'the' from the tokens because the most of the frequenciest words were stop words. Accuracy was 65.84%.\n",
        "\n",
        "In the third part:\n",
        "* **Different Embedding Size :** I have tried with [100, 200, 300, 1000, 3000] embedding sizes. The best one is gotten with embedding_size = 1000.\n",
        "* **Reducing Vocabulary Size :** I have tried with most frequent [1000, 2000, 3000, 10000, 30000,100000] words. And the best accuracies are gotten with 10000 and 100000. However, training time has increased when vocabulary size increased. Therefore, I have choosen 10000 as best vocabulary size for this dataset.\n",
        "* **Pretrained Weights :** With the pretrained weights, accuracy has increased significantly, and because of already pretrained weights training time has decreased. Therefore, using pretrained weigths has very good results for this dataset.\n",
        "* **Change Depth of the Network :** When the depth of the network has increased, accuracy also increased for pretrained weigths. But increase on the accuracy was very low for using it because it also increased training time. However if the pretrained results were not used, increase on the depth would impact on accuracy significantly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njw5fulo7613"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNN8CFoRBBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec0d6eb-6a86-4c64-ce18-14ff3534707f"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__) #version of the pytorh\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy as torchtext\n",
        "import random\n",
        "\n",
        "## Fill Here ##   Add any library that is needed for preprocessing steps\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import nltk # in order to stemming and lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer # in order to delete punctuations\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRW0mcRQn_PK"
      },
      "source": [
        "### Finding Max Length \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9juFZANBZEqX"
      },
      "source": [
        "First thing we need to do is finding the max length of the text in the dataset because our input size is constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz3mTFB3OOAo"
      },
      "source": [
        "## In this cell create \"tokenize_fn\" that takes text and returns tokens\n",
        "##Example\n",
        "## def tokenize_fn(text):\n",
        "##   tokens = do_tokenize(text)\n",
        "##   return tokens\n",
        "\n",
        "##Fill here##\n",
        "\n",
        "# I have choosed \"basic english\" as tokenizer because process with \"spacy\" was taking so long.\n",
        "def tokenize_fn(text):\n",
        "  tokenizer = get_tokenizer(\"basic_english\")\n",
        "  tokens = tokenizer(text)\n",
        "  return tokens"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Ni6oXTich-"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PEQVuI_m3uV"
      },
      "source": [
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac25bdxVXecP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a22a467-068e-4da4-c5e9-d5970485d57f"
      },
      "source": [
        "max_size=0  ## this part of the code find maximum length of the network\n",
        "count=0\n",
        "sum= 0\n",
        "for i in  range(len(train_data)):\n",
        "  if max_size < len(train_data[i].text):\n",
        "    max_size =len(train_data[i].text)\n",
        "    print(max_size)\n",
        "  count +=1\n",
        "  sum +=len(train_data[i].text)\n",
        "print(\"avarage: \", sum/count)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "206\n",
            "712\n",
            "1086\n",
            "1096\n",
            "1128\n",
            "1160\n",
            "1299\n",
            "2022\n",
            "2752\n",
            "avarage:  270.68748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7I6kZ7oHKX"
      },
      "source": [
        "### Training Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy6YWYP4oN58"
      },
      "source": [
        "We just have found the max length corpus. Now let's create our train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_wJl9cwmuR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2410aa97-141b-41fb-eb42-bf2f660c24e1"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize=tokenize_fn, batch_first=True,fix_length= max_size # it is filled with max size \n",
        "                            ) # preprocessing parameters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL) \n",
        "print(\"train length is: \",len(train_data))\n",
        "print(\"test length is: \",len(test_data))\n",
        "print(vars(train_data[0]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['alex', 'winter', 'and', 'keanu', 'reeves', 'return', 'as', 'the', 'two', 'dopes', 'from', 'san', 'dimas', 'who', 'get', 'sent', 'on', 'another', 'trip', 'of', 'a', 'lifetime', 'as', 'someone', 'from', 'the', 'future', 'feels', 'exactly', 'the', 'opposite', 'the', 'way', 'it', 'was', 'presented', 'in', 'the', 'first', 'movie', '.', 'the', 'only', 'difference', 'is', 'that', 'their', 'trip', 'is', 'somewhere', 'between', 'heaven', 'and', 'hell', 'and', 'ends', 'up', 'being', 'both', '.', 'when', 'they', 'meet', 'the', 'grim', 'reaper', ',', 'they', 'get', 'the', 'chance', 'of', 'an', 'after-lifetime', 'to', 'play', 'him', 'for', 'a', 'chance', 'to', 'return', 'and', 'stop', 'two', 'evil', 'robots', 'from', 'ruining', 'what', 'future', 'they', 'were', 'supposed', 'to', 'have', '.', 'besides', 'playing', 'roles', 'they', 'have', '.', '.', '.', 'er', '.', '.', '.', 'perfected', ',', 'they', 'also', 'play', '(', 'and', 'revive', 'a', 'couple', 'of', 'extra', 'sales', 'in', 'the', 'process', ')', 'some', 'classic', 'games', '(', 'i', 'even', 'have', 'my', 'original', 'copy', 'of', 'battleship', 'in', 'the', 'closet', ')', '.', 'the', 'reason', 'i', 'liked', 'this', 'movie', 'better', 'than', 'the', 'original', 'is', 'because', 'it', 'deals', 'with', 'what', 'it', 'might', 'be', 'like', 'instead', 'of', 'what', 'was', '.', 'without', 'spoiling', 'the', 'movie', ',', 'i', 'can', \"'\", 't', 'give', 'you', 'anymore', 'information', 'about', 'this', '(', 'i', 'guess', 'you', \"'\", 'll', 'just', 'have', 'to', 'watch', 'them', 'both', 'and', 'decide', 'for', 'yourself', '!', '8', 'out', 'of', '10', 'stars', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1Bthmaj1Lr"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOLkG_dgjxH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb7182c-5b61-4d3c-93fc-0db94078cbf6"
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\n",
        "print(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(LABEL.vocab.freqs)\n",
        "print(TEXT.unk_token)\n",
        "print(TEXT.pad_token)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 100684\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "[('the', 335746), ('.', 327192), (',', 276280), ('and', 163290), ('a', 162473), ('of', 145437), ('to', 135208), (\"'\", 133857), ('is', 107221), ('it', 96024), ('in', 93307), ('i', 87401), ('this', 75878), ('that', 73153), ('s', 62933), ('was', 48170), ('as', 46807), ('for', 44116), ('with', 44041), ('movie', 43421)]\n",
            "Counter({'pos': 12500, 'neg': 12500})\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9PiSpoDmuy-"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data, test_data), batch_size=32, device=device)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRCXsKl5-u3e"
      },
      "source": [
        "Let's create our network;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCxdM6joBbZ"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x       "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENDe2sd5rJn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d53667-0f68-48cf-b412-b2aa6bc31e50"
      },
      "source": [
        "model = Network(pad_idx = TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(model)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(100684, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=275200, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5B5TDLdqi93"
      },
      "source": [
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0Fw71NqNgD"
      },
      "source": [
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJUAGG5EDYJo"
      },
      "source": [
        "We can train our network now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFvTmIyXGyR"
      },
      "source": [
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "  accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "  return accuracy"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVmtOVeVQJoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47246f9-5d1b-430f-b117-d4a082fd80c8"
      },
      "source": [
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()  \n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "        \n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 0m 14s\n",
            "\tAvarage Train Loss: 0.672 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 0m 14s\n",
            "\tAvarage Train Loss: 0.452 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCdD1EBy4W8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1f7980-8e25-4627-d43f-6e25ba551275"
      },
      "source": [
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss = test_epoch_loss/len(test_iterator)\n",
        "test_acc = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.649 | | Test Acc: 63.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcUrjVXhhDN"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEZ2bIE0pXgt"
      },
      "source": [
        "### Part2-A : With Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrExixVLv_r6"
      },
      "source": [
        "def my_stemmer(text):\n",
        "  stems = []\n",
        "  stemmer = nltk.stem.SnowballStemmer('english') \n",
        "  for token in text:\n",
        "    token = stemmer.stem(token)\n",
        "    if token != \"\":\n",
        "      stems.append(token)\n",
        "  return stems "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXYLZ7mAt5GS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2c8a52-7994-4427-8922-4be0c05eaa1b"
      },
      "source": [
        "TEXT_STEM = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True, preprocessing = my_stemmer, fix_length = max_size) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL_STEM = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data_stemming, test_data_stemming = torchtext.datasets.IMDB.splits(TEXT_STEM, LABEL_STEM) \n",
        "print(\"train length is: \",len(train_data_stemming))\n",
        "print(\"test length is: \",len(test_data_stemming))\n",
        "print(vars(train_data_stemming[0]))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['alex', 'winter', 'and', 'keanu', 'reev', 'return', 'as', 'the', 'two', 'dope', 'from', 'san', 'dima', 'who', 'get', 'sent', 'on', 'anoth', 'trip', 'of', 'a', 'lifetim', 'as', 'someon', 'from', 'the', 'futur', 'feel', 'exact', 'the', 'opposit', 'the', 'way', 'it', 'was', 'present', 'in', 'the', 'first', 'movi', '.', 'the', 'onli', 'differ', 'is', 'that', 'their', 'trip', 'is', 'somewher', 'between', 'heaven', 'and', 'hell', 'and', 'end', 'up', 'be', 'both', '.', 'when', 'they', 'meet', 'the', 'grim', 'reaper', ',', 'they', 'get', 'the', 'chanc', 'of', 'an', 'after-lifetim', 'to', 'play', 'him', 'for', 'a', 'chanc', 'to', 'return', 'and', 'stop', 'two', 'evil', 'robot', 'from', 'ruin', 'what', 'futur', 'they', 'were', 'suppos', 'to', 'have', '.', 'besid', 'play', 'role', 'they', 'have', '.', '.', '.', 'er', '.', '.', '.', 'perfect', ',', 'they', 'also', 'play', '(', 'and', 'reviv', 'a', 'coupl', 'of', 'extra', 'sale', 'in', 'the', 'process', ')', 'some', 'classic', 'game', '(', 'i', 'even', 'have', 'my', 'origin', 'copi', 'of', 'battleship', 'in', 'the', 'closet', ')', '.', 'the', 'reason', 'i', 'like', 'this', 'movi', 'better', 'than', 'the', 'origin', 'is', 'becaus', 'it', 'deal', 'with', 'what', 'it', 'might', 'be', 'like', 'instead', 'of', 'what', 'was', '.', 'without', 'spoil', 'the', 'movi', ',', 'i', 'can', \"'\", 't', 'give', 'you', 'anymor', 'inform', 'about', 'this', '(', 'i', 'guess', 'you', \"'\", 'll', 'just', 'have', 'to', 'watch', 'them', 'both', 'and', 'decid', 'for', 'yourself', '!', '8', 'out', 'of', '10', 'star', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYcY2T7YwZGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c6a0db-a608-4957-9da3-a789f6f74239"
      },
      "source": [
        "TEXT_STEM.build_vocab(train_data_stemming)\n",
        "LABEL_STEM.build_vocab(train_data_stemming)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data_stemming, test_data_stemming), batch_size=32, device=device)\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT_STEM.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x       \n",
        "\n",
        "model = Network(pad_idx = TEXT_STEM.vocab.stoi[TEXT_STEM.pad_token])\n",
        "print(model)\n",
        "\n",
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "  accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "  return accuracy\n",
        "\n",
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()  \n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') \n",
        "\n",
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss_stem = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss_stem.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss_stem = test_epoch_loss/len(test_iterator)\n",
        "test_acc_stem = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss_stem:.3f} | | Test Acc: {test_acc_stem*100:.2f}%')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(75641, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=275200, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 0m 12s\n",
            "\tAvarage Train Loss: 0.667 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 0m 13s\n",
            "\tAvarage Train Loss: 0.447 \n",
            "\n",
            "\n",
            "Test Loss: 0.646 | | Test Acc: 64.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Zy4XdMV_4R"
      },
      "source": [
        "### Part2-B: With Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltJhO_0u1hDe"
      },
      "source": [
        "def my_lemmatization(text):\n",
        "  lemmas = []\n",
        "  lemmatize = nltk.stem.WordNetLemmatizer() \n",
        "  for token in text:\n",
        "    token = lemmatize.lemmatize(token)\n",
        "    if token != \"\":\n",
        "      lemmas.append(token)\n",
        "  return lemmas "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uL23jejWZUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc452f42-83cb-40fe-80c4-6cfd85fec6fe"
      },
      "source": [
        "TEXT_LEMMA = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True, preprocessing = my_lemmatization, fix_length = max_size) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL_LEMMA = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data_lemma, test_data_lemma = torchtext.datasets.IMDB.splits(TEXT_LEMMA, LABEL_LEMMA) \n",
        "print(\"train length is: \",len(train_data_lemma))\n",
        "print(\"test length is: \",len(test_data_lemma))\n",
        "print(vars(train_data_lemma[0]))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['alex', 'winter', 'and', 'keanu', 'reef', 'return', 'a', 'the', 'two', 'dope', 'from', 'san', 'dimas', 'who', 'get', 'sent', 'on', 'another', 'trip', 'of', 'a', 'lifetime', 'a', 'someone', 'from', 'the', 'future', 'feel', 'exactly', 'the', 'opposite', 'the', 'way', 'it', 'wa', 'presented', 'in', 'the', 'first', 'movie', '.', 'the', 'only', 'difference', 'is', 'that', 'their', 'trip', 'is', 'somewhere', 'between', 'heaven', 'and', 'hell', 'and', 'end', 'up', 'being', 'both', '.', 'when', 'they', 'meet', 'the', 'grim', 'reaper', ',', 'they', 'get', 'the', 'chance', 'of', 'an', 'after-lifetime', 'to', 'play', 'him', 'for', 'a', 'chance', 'to', 'return', 'and', 'stop', 'two', 'evil', 'robot', 'from', 'ruining', 'what', 'future', 'they', 'were', 'supposed', 'to', 'have', '.', 'besides', 'playing', 'role', 'they', 'have', '.', '.', '.', 'er', '.', '.', '.', 'perfected', ',', 'they', 'also', 'play', '(', 'and', 'revive', 'a', 'couple', 'of', 'extra', 'sale', 'in', 'the', 'process', ')', 'some', 'classic', 'game', '(', 'i', 'even', 'have', 'my', 'original', 'copy', 'of', 'battleship', 'in', 'the', 'closet', ')', '.', 'the', 'reason', 'i', 'liked', 'this', 'movie', 'better', 'than', 'the', 'original', 'is', 'because', 'it', 'deal', 'with', 'what', 'it', 'might', 'be', 'like', 'instead', 'of', 'what', 'wa', '.', 'without', 'spoiling', 'the', 'movie', ',', 'i', 'can', \"'\", 't', 'give', 'you', 'anymore', 'information', 'about', 'this', '(', 'i', 'guess', 'you', \"'\", 'll', 'just', 'have', 'to', 'watch', 'them', 'both', 'and', 'decide', 'for', 'yourself', '!', '8', 'out', 'of', '10', 'star', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607qq1PqZA2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010dbe6d-1739-48e1-db49-bee0cc63b626"
      },
      "source": [
        "TEXT_LEMMA.build_vocab(train_data_lemma)\n",
        "LABEL_LEMMA.build_vocab(train_data_lemma)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data_lemma, test_data_lemma), batch_size=32, device=device)\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT_LEMMA.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x       \n",
        "\n",
        "model = Network(pad_idx = TEXT_LEMMA.vocab.stoi[TEXT_LEMMA.pad_token])\n",
        "print(model)\n",
        "\n",
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "  accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "  return accuracy\n",
        "\n",
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()  \n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time with Lemmatization: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss with Lemmatization: {train_loss:.3f} ')\n",
        "    print('\\n') \n",
        "\n",
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss_lemma = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss_lemma.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss_lemma = test_epoch_loss/len(test_iterator)\n",
        "test_acc_lemma = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss with Lemmatization: {test_loss_lemma:.3f} | | Test Acc with Lemmatization: {test_acc_lemma*100:.2f}%')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(93055, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=275200, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time with Lemmatization: 0m 13s\n",
            "\tAvarage Train Loss with Lemmatization: 0.675 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time with Lemmatization: 0m 13s\n",
            "\tAvarage Train Loss with Lemmatization: 0.458 \n",
            "\n",
            "\n",
            "Test Loss with Lemmatization: 0.657 | | Test Acc with Lemmatization: 63.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMewE_K1azoH"
      },
      "source": [
        "### Part2-C: Lemmatization and removing stop word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFsOwCRJm1fX"
      },
      "source": [
        "This module deletes stop words, lemmatize the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdjBJ-U5ZTOT"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def my_lemmatization_and_remove_stop_words(text):\n",
        "  lemmas = []\n",
        "  lemmatize = nltk.stem.WordNetLemmatizer() \n",
        "\n",
        "  for token in text:\n",
        "    token = lemmatize.lemmatize(token)\n",
        "    if token != \"\":\n",
        "      lemmas.append(token)\n",
        "  lemmas = [word for word in lemmas if not word in stop_words] \n",
        "  return lemmas "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF9aDmOma_K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f836d62e-6b55-4d3e-f2e2-7654ade9920d"
      },
      "source": [
        "TEXT_C = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True, preprocessing = my_lemmatization_and_remove_stop_words) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL_C = torchtext.data.LabelField(dtype = torch.float)\n",
        "\n",
        "train_data_C, test_data_lemma = torchtext.datasets.IMDB.splits(TEXT_C, LABEL_C)\n",
        "\n",
        "max_size=0  ## this part of the code find maximum length of the network\n",
        "count=0\n",
        "sum= 0\n",
        "for i in  range(len(train_data_C)):\n",
        "  if max_size < len(train_data_C[i].text):\n",
        "    max_size =len(train_data_C[i].text)\n",
        "    print(max_size)\n",
        "  count +=1\n",
        "  sum +=len(train_data_C[i].text)\n",
        "print(\"avarage: \", sum/count)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "110\n",
            "364\n",
            "391\n",
            "584\n",
            "591\n",
            "642\n",
            "653\n",
            "676\n",
            "778\n",
            "1170\n",
            "1700\n",
            "avarage:  156.94716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31s3ysvbBJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa8f7f5-5917-4b3b-d56b-a67474165b40"
      },
      "source": [
        "TEXT_C = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True, preprocessing = my_lemmatization_and_remove_stop_words, fix_length = max_size) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL_C = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data_C, test_data_C = torchtext.datasets.IMDB.splits(TEXT_C, LABEL_C) \n",
        "print(\"train length is: \",len(train_data_C))\n",
        "print(\"test length is: \",len(test_data_C))\n",
        "print(vars(train_data_C[0]))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['alex', 'winter', 'keanu', 'reef', 'return', 'two', 'dope', 'san', 'dimas', 'get', 'sent', 'another', 'trip', 'lifetime', 'someone', 'future', 'feel', 'exactly', 'opposite', 'way', 'wa', 'presented', 'first', 'movie', '.', 'difference', 'trip', 'somewhere', 'heaven', 'hell', 'end', '.', 'meet', 'grim', 'reaper', ',', 'get', 'chance', 'after-lifetime', 'play', 'chance', 'return', 'stop', 'two', 'evil', 'robot', 'ruining', 'future', 'supposed', '.', 'besides', 'playing', 'role', '.', '.', '.', 'er', '.', '.', '.', 'perfected', ',', 'also', 'play', '(', 'revive', 'couple', 'extra', 'sale', 'process', ')', 'classic', 'game', '(', 'even', 'original', 'copy', 'battleship', 'closet', ')', '.', 'reason', 'liked', 'movie', 'better', 'original', 'deal', 'might', 'like', 'instead', 'wa', '.', 'without', 'spoiling', 'movie', ',', \"'\", 'give', 'anymore', 'information', '(', 'guess', \"'\", 'watch', 'decide', '!', '8', '10', 'star', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_d53SB5bDFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af509be-6c56-4d82-f0e2-3609a94bf7c6"
      },
      "source": [
        "TEXT_C.build_vocab(train_data_C)\n",
        "LABEL_C.build_vocab(train_data_C)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data_C, test_data_C), batch_size=32, device=device)\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT_C.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x       \n",
        "\n",
        "model = Network(pad_idx = TEXT_C.vocab.stoi[TEXT_C.pad_token])\n",
        "print(model)\n",
        "\n",
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "  accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "  return accuracy\n",
        "\n",
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()  \n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time on Part2-C: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss on Part2-C: {train_loss:.3f} ')\n",
        "    print('\\n') \n",
        "\n",
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss_C = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss_C.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss_C = test_epoch_loss/len(test_iterator)\n",
        "test_acc_C = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_C*100:.2f}%')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(92906, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=170000, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 9s\n",
            "\tAvarage Train Loss on Part2-C: 0.668 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 9s\n",
            "\tAvarage Train Loss on Part2-C: 0.499 \n",
            "\n",
            "\n",
            "Test Loss on Part2-C: 0.616 | | Test Acc on Part2-C: 66.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6gy0RCsvhS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "546db671-32ff-4560-c8eb-29c197df536c"
      },
      "source": [
        "left = [1, 2, 3, 4]\n",
        "  \n",
        "height = [test_acc, test_acc_stem, test_acc_lemma, test_acc_C]\n",
        "  \n",
        "# labels for bars\n",
        "tick_label = ['Test_Accuracy for First Part', 'Test_Accuracy with Stemming', 'Test_Accuracy with Lemmatization', 'Test_Accuracy with Lemmatization-Rm Stop Words and Marking Neg. Words']\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# plotting a bar chart\n",
        "plt.bar(left, height, tick_label = tick_label,\n",
        "        width = 0.8, color = ['red', 'green'])\n",
        "plt.ylim([0.6, 0.7])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of the Models\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy of the Models')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8nriAOyX0d"
      },
      "source": [
        "As can be seen from the Figure I found the best accuracy from : Part2-C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwroJnUchT-J"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkwwMUOSoFL5"
      },
      "source": [
        "### With Different Embedding Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y0y8-UFaB7i"
      },
      "source": [
        "TEXT_C = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True, preprocessing = my_lemmatization_and_remove_stop_words, fix_length = max_size) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL_C = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data_C, test_data_C = torchtext.datasets.IMDB.splits(TEXT_C, LABEL_C) "
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfNRm5r61hFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "6f310273-4be6-4948-c1dd-8ba7332f52b8"
      },
      "source": [
        "TEXT_C.build_vocab(train_data_C)\n",
        "LABEL_C.build_vocab(train_data_C)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data_C, test_data_C), batch_size=32, device=device)\n",
        "emb_dims = [100, 200, 300, 1000, 3000]\n",
        "for embedding_dims in emb_dims:\n",
        "  class Network(torch.nn.Module):\n",
        "      def __init__(self,pad_idx):\n",
        "          super().__init__()\n",
        "          self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT_C.vocab), embedding_dim =embedding_dims,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "          self.layer1 = torch.nn.Linear(max_size*embedding_dims,1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.embedding(x).view(x.size(0),-1)\n",
        "          x = self.layer1(x)\n",
        "          return x       \n",
        "\n",
        "  model = Network(pad_idx = TEXT_C.vocab.stoi[TEXT_C.pad_token])\n",
        "  #print(model)\n",
        "\n",
        "  # Choose a Loss function from torch.nn according to your network\n",
        "  loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "  #Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "  optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##\n",
        "\n",
        "  model = model.to(device)\n",
        "  loss_fn = loss_fn.to(device)\n",
        "\n",
        "  def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "    correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "    accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "    return accuracy\n",
        "\n",
        "  import time\n",
        "  # Training loop\n",
        "  N_EPOCHS = 2\n",
        "\n",
        "  tr_loss = []\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      \n",
        "      # Calculate training time\n",
        "      start_time = time.time()\n",
        "\n",
        "      epoch_loss = 0\n",
        "      epoch_acc = 0\n",
        "\n",
        "      \n",
        "      batch_no = 0\n",
        "      for batch in train_iterator:\n",
        "          predictions = model(batch.text).squeeze(1)\n",
        "          loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "          ###Fill Here###\n",
        "          # Reset the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Backprop \n",
        "          loss.backward()  \n",
        "          # Optimize the weights\n",
        "          optimizer.step()\n",
        "          ##################################\n",
        "\n",
        "          # Record accuracy and loss\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "          epoch_acc +=acc.item()\n",
        "\n",
        "          batch_no = batch_no +1\n",
        "\n",
        "      \n",
        "      train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      elapsed_time = end_time - start_time\n",
        "      elapsed_mins = int(elapsed_time / 60)\n",
        "      elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "      \n",
        "      #print('\\n')    \n",
        "      print(f'Epoch: {epoch+1:2} | Epoch Time on Part2-C: {elapsed_mins}m {elapsed_secs}s')\n",
        "      #print(f'\\tAvarage Train Loss on Part2-C: {train_loss:.3f} ')\n",
        "      #print('\\n') \n",
        "\n",
        "  test_epoch_loss = 0\n",
        "  test_epoch_acc = 0\n",
        "\n",
        "  # Turm on evalutaion mode\n",
        "  model.eval()\n",
        "\n",
        "  # No need to backprop in eval\n",
        "  with torch.no_grad():\n",
        "\n",
        "      for batch in test_iterator:\n",
        "\n",
        "          test_predictions = model(batch.text).squeeze(1)\n",
        "          \n",
        "          test_loss_C = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "          test_epoch_loss += test_loss_C.item()\n",
        "          \n",
        "          acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "          test_epoch_acc +=acc.item()\n",
        "\n",
        "  test_loss_C = test_epoch_loss/len(test_iterator)\n",
        "  test_acc_C = test_epoch_acc  / len(test_iterator)\n",
        "  if embedding_dims == 100:\n",
        "    test_acc_100 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Embedding Size: {embedding_dims} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_100*100:.2f}%')\n",
        "  if embedding_dims == 200:\n",
        "    test_acc_200 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Embedding Size: {embedding_dims} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_200*100:.2f}%')\n",
        "  if embedding_dims == 300:\n",
        "    test_acc_300 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Embedding Size: {embedding_dims} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_300*100:.2f}%')\n",
        "  if embedding_dims == 1000:\n",
        "    test_acc_1000 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Embedding Size: {embedding_dims} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_1000*100:.2f}%')\n",
        "  if embedding_dims == 3000:\n",
        "    test_acc_3000 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Embedding Size: {embedding_dims} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_3000*100:.2f}%')\n",
        "    \n",
        "left = [1, 2, 3, 4, 5]\n",
        "  \n",
        "height = [test_acc_100, test_acc_200, test_acc_300, test_acc_1000, test_acc_3000]\n",
        "  \n",
        "# labels for bars\n",
        "tick_label = ['Test_Accuracy for Emb_Dim = 100', 'Test_Accuracy for Emb_Dim = 200', 'Test_Accuracy for Emb_Dim = 300', 'Test_Accuracy for Emb_Dim = 1000', 'Test_Accuracy for Emb_Dim = 3000']\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# plotting a bar chart\n",
        "plt.bar(left, height, tick_label = tick_label,\n",
        "        width = 0.8, color = ['red', 'green'])\n",
        "plt.ylim([0.6, 0.7])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of the Models\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 | Epoch Time on Part2-C: 0m 9s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 9s\n",
            "Embedding Size: 100 || Test Loss on Part2-C: 0.639 | | Test Acc on Part2-C: 63.81%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 13s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 12s\n",
            "Embedding Size: 200 || Test Loss on Part2-C: 0.622 | | Test Acc on Part2-C: 66.74%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 16s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 16s\n",
            "Embedding Size: 300 || Test Loss on Part2-C: 0.615 | | Test Acc on Part2-C: 67.66%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 41s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 41s\n",
            "Embedding Size: 1000 || Test Loss on Part2-C: 0.662 | | Test Acc on Part2-C: 70.55%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 1m 50s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 1m 50s\n",
            "Embedding Size: 3000 || Test Loss on Part2-C: 1.143 | | Test Acc on Part2-C: 69.02%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy of the Models')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOmmxjQ7Mzc"
      },
      "source": [
        "*   When the embedding size has increased, also training time is increased.\n",
        "*   As can be seen from the Figure, Embedding Size with 1000 has increased Test_Accuracy very much. Therefore, the following parts will be trained with embedding_Size = 1000. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_8pmU175mG"
      },
      "source": [
        "### Eliminating Less Frequent Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz0PaPnr8HUp"
      },
      "source": [
        "Max vocabulary size of the text vocab is set to different sizes in order to get rid of less frequently used words and their accuracy will be compared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwLFSyDT1yLV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "outputId": "d1188889-8456-45ea-dbd6-43052da60e67"
      },
      "source": [
        "max_vocab_sizes = [1000, 2000, 3000, 10000, 30000,100000]\n",
        "for max_sizes in max_vocab_sizes:\n",
        "  TEXT_C.build_vocab(train_data_C, max_size = max_sizes)\n",
        "  LABEL_C.build_vocab(train_data_C)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  ##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "  train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data_C, test_data_C), batch_size=32, device=device)\n",
        "  class Network(torch.nn.Module):\n",
        "      def __init__(self,pad_idx):\n",
        "          super().__init__()\n",
        "          self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT_C.vocab), embedding_dim =1000,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "          self.layer1 = torch.nn.Linear(max_size*1000,1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.embedding(x).view(x.size(0),-1)\n",
        "          x = self.layer1(x)\n",
        "          return x       \n",
        "\n",
        "  model = Network(pad_idx = TEXT_C.vocab.stoi[TEXT_C.pad_token])\n",
        "  #print(model)\n",
        "\n",
        "  # Choose a Loss function from torch.nn according to your network\n",
        "  loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "  #Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "  optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.0001)  ## Fill here##\n",
        "\n",
        "  model = model.to(device)\n",
        "  loss_fn = loss_fn.to(device)\n",
        "\n",
        "  def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "    correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n",
        "    accuracy = correct.sum() / len(correct)  ##Fill here\n",
        "    return accuracy\n",
        "\n",
        "  import time\n",
        "  # Training loop\n",
        "  N_EPOCHS = 2\n",
        "\n",
        "  tr_loss = []\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      \n",
        "      # Calculate training time\n",
        "      start_time = time.time()\n",
        "\n",
        "      epoch_loss = 0\n",
        "      epoch_acc = 0\n",
        "\n",
        "      \n",
        "      batch_no = 0\n",
        "      for batch in train_iterator:\n",
        "          predictions = model(batch.text).squeeze(1)\n",
        "          loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "          ###Fill Here###\n",
        "          # Reset the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Backprop \n",
        "          loss.backward()  \n",
        "          # Optimize the weights\n",
        "          optimizer.step()\n",
        "          ##################################\n",
        "\n",
        "          # Record accuracy and loss\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "          epoch_acc +=acc.item()\n",
        "\n",
        "          batch_no = batch_no +1\n",
        "\n",
        "      \n",
        "      train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      elapsed_time = end_time - start_time\n",
        "      elapsed_mins = int(elapsed_time / 60)\n",
        "      elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "      \n",
        "      #print('\\n')    \n",
        "      print(f'Epoch: {epoch+1:2} | Epoch Time on Part2-C: {elapsed_mins}m {elapsed_secs}s')\n",
        "      #print(f'\\tAvarage Train Loss on Part2-C: {train_loss:.3f} ')\n",
        "      #print('\\n') \n",
        "\n",
        "  test_epoch_loss = 0\n",
        "  test_epoch_acc = 0\n",
        "\n",
        "  # Turm on evalutaion mode\n",
        "  model.eval()\n",
        "\n",
        "  # No need to backprop in eval\n",
        "  with torch.no_grad():\n",
        "\n",
        "      for batch in test_iterator:\n",
        "\n",
        "          test_predictions = model(batch.text).squeeze(1)\n",
        "          \n",
        "          test_loss_C = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "          test_epoch_loss += test_loss_C.item()\n",
        "          \n",
        "          acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "          test_epoch_acc +=acc.item()\n",
        "\n",
        "  test_loss_C = test_epoch_loss/len(test_iterator)\n",
        "  test_acc_C = test_epoch_acc  / len(test_iterator)\n",
        "  if max_sizes == 1000:\n",
        "    test_acc_100 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_100*100:.2f}%')\n",
        "  if max_sizes == 2000:\n",
        "    test_acc_200 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_200*100:.2f}%')\n",
        "  if max_sizes == 3000:\n",
        "    test_acc_300 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_300*100:.2f}%')\n",
        "  if max_sizes == 10000:\n",
        "    test_acc_1000 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_1000*100:.2f}%')\n",
        "  if max_sizes == 30000:\n",
        "    test_acc_3000 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_3000*100:.2f}%')\n",
        "  if max_sizes == 100000:\n",
        "    test_acc_10000 = test_epoch_acc  / len(test_iterator)\n",
        "    print(f'Max Vocabulary Size: {max_sizes} || Test Loss on Part2-C: {test_loss_C:.3f} | | Test Acc on Part2-C: {test_acc_10000*100:.2f}%')\n",
        "    \n",
        "left = [1, 2, 3, 4, 5, 6]\n",
        "  \n",
        "height = [test_acc_100, test_acc_200, test_acc_300, test_acc_1000, test_acc_3000, test_acc_10000]\n",
        "  \n",
        "# labels for bars\n",
        "tick_label = ['Test_Accuracy for Max Vocabulary Size = 1000', 'Test_Accuracy for Max Vocabulary Size = 2000', 'Test_Accuracy for Max Vocabulary Size = 3000', 'Test_Accuracy for Max Vocabulary Size = 10000', 'Test_Accuracy for Max Vocabulary Size = 30000','Test_Accuracy for Max Vocabulary Size = 100000']\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# plotting a bar chart\n",
        "plt.bar(left, height, tick_label = tick_label,\n",
        "        width = 0.8, color = ['red', 'green'])\n",
        "plt.ylim([0.64, 0.7])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of the Models\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1 | Epoch Time on Part2-C: 0m 11s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 11s\n",
            "Max Vocabulary Size: 1000 || Test Loss on Part2-C: 0.837 | | Test Acc on Part2-C: 68.86%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 11s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 11s\n",
            "Max Vocabulary Size: 2000 || Test Loss on Part2-C: 0.784 | | Test Acc on Part2-C: 69.37%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 11s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 11s\n",
            "Max Vocabulary Size: 3000 || Test Loss on Part2-C: 0.759 | | Test Acc on Part2-C: 69.50%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 14s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 14s\n",
            "Max Vocabulary Size: 10000 || Test Loss on Part2-C: 0.728 | | Test Acc on Part2-C: 68.97%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 20s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 21s\n",
            "Max Vocabulary Size: 30000 || Test Loss on Part2-C: 0.683 | | Test Acc on Part2-C: 70.08%\n",
            "Epoch:  1 | Epoch Time on Part2-C: 0m 41s\n",
            "Epoch:  2 | Epoch Time on Part2-C: 0m 41s\n",
            "Max Vocabulary Size: 100000 || Test Loss on Part2-C: 0.676 | | Test Acc on Part2-C: 69.72%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy of the Models')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MojPXMxhDrjA"
      },
      "source": [
        "\n",
        "\n",
        "*   As can be seen from the figure, vocabulary size with 3000 and 10000 gave the best results. However traninig time for 3000 words is shorter, therefore max_vocab size is set to 3000.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd0B8bONLv79"
      },
      "source": [
        "### Pretrained Weigths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvIEKjc6Lus0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c2e207-864c-44f4-8c47-ef75d1ff8084"
      },
      "source": [
        "TEXT_C.build_vocab(train_data_C,\n",
        "                 max_size = 3000,\n",
        "                  vectors = \"glove.6B.100d\", \n",
        "                 # Set unknown vectors\n",
        "                  unk_init = torch.Tensor.normal_)\n",
        "LABEL_C.build_vocab(train_data_C)\n",
        "print(\"Unique tokens in TEXT vocabulary:\",len(TEXT_C.vocab))\n",
        "print(\"Unique tokens in LABEL vocabulary:\",len(LABEL_C.vocab))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 3002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmoGDwIjMcza"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_data_C, test_data_C), batch_size=32, device=device)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUNat8hMioL"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding =torch.nn.Embedding(num_embeddings = len(TEXT_C.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100, 1)##Fill Here ##\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x     "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snIkKE4RMna4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3637bf9d-ad82-4d97-c87f-b90e9b0689a2"
      },
      "source": [
        "model = Network(pad_idx = TEXT_C.vocab.stoi[TEXT.pad_token])\n",
        "print(model)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(3002, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=170000, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZCMMKA8MqfL"
      },
      "source": [
        "model.embedding.weight.data.copy_(TEXT_C.vocab.vectors)\n",
        "model.embedding.weight.data[TEXT_C.vocab.stoi[TEXT_C.unk_token]] = torch.zeros(100)\n",
        "model.embedding.weight.data[TEXT_C.vocab.stoi[TEXT_C.pad_token]] = torch.zeros(100)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgBmKbiEM0uo"
      },
      "source": [
        "model.embedding.requires_grad = False"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v2IVM3RM2Dx"
      },
      "source": [
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.001) ## Fill here##"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5C1tbh0M3eW"
      },
      "source": [
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Col6KJFPM46n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1e0583-9ae7-42d8-9806-7cd0a3552fe3"
      },
      "source": [
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()\n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 0m 7s\n",
            "\tAvarage Train Loss: 0.539 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 0m 7s\n",
            "\tAvarage Train Loss: 0.240 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZt1lWz9M_OW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b49d3fe-e9e6-4fe8-fc69-27f74fad6ed4"
      },
      "source": [
        "## Pretrained weights test\n",
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss = test_epoch_loss/len(test_iterator)\n",
        "test_acc_pretrain = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc_pretrain*100:.2f}%')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.414 | | Test Acc: 83.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8nnUTXtQ1rl"
      },
      "source": [
        "Pretrained weigths have been increased accuracy very much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b-PoR05RRuY"
      },
      "source": [
        "### Change Depth of the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzVpGx7lRPyn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "5e5e2a14-c986-40a4-c022-ee5dfa55712b"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding =torch.nn.Embedding(num_embeddings = len(TEXT_C.vocab), embedding_dim =100,padding_idx = pad_idx)##Fill Here ## create an embedding layer with 100 size\n",
        "        self.layer1 = torch.nn.Linear(max_size*100, 250)##Fill Here ##\n",
        "        self.layer2 = torch.nn.Linear(250,100)\n",
        "        self.layer3 = torch.nn.Linear(100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x     \n",
        "\n",
        "model = Network(pad_idx = TEXT_C.vocab.stoi[TEXT.pad_token])\n",
        "print(model)\n",
        "\n",
        "model.embedding.weight.data.copy_(TEXT_C.vocab.vectors)\n",
        "model.embedding.weight.data[TEXT_C.vocab.stoi[TEXT_C.unk_token]] = torch.zeros(100)\n",
        "model.embedding.weight.data[TEXT_C.vocab.stoi[TEXT_C.pad_token]] = torch.zeros(100)\n",
        "\n",
        "model.embedding.requires_grad = False\n",
        "\n",
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()##Fill here## \n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.001) ## Fill here##\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "        ###Fill Here###\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Backprop \n",
        "        loss.backward()\n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') \n",
        "\n",
        "## Pretrained weights test\n",
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss = test_epoch_loss/len(test_iterator)\n",
        "test_acc_pretrain_deep = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc_pretrain_deep*100:.2f}%')\n",
        "\n",
        "left = [1, 2]\n",
        "  \n",
        "height = [test_acc_pretrain, test_acc_pretrain_deep]\n",
        "  \n",
        "# labels for bars\n",
        "tick_label = ['Test Accuracy with Pretrained - 1 Layer','Test Accuracy with Pretrained - 3 Layer']\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# plotting a bar chart\n",
        "plt.bar(left, height, tick_label = tick_label,\n",
        "        width = 0.8, color = ['red', 'green'])\n",
        "plt.ylim([0.82, 0.85])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of the Models\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(3002, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=170000, out_features=250, bias=True)\n",
            "  (layer2): Linear(in_features=250, out_features=100, bias=True)\n",
            "  (layer3): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 0m 22s\n",
            "\tAvarage Train Loss: 0.531 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 0m 22s\n",
            "\tAvarage Train Loss: 0.250 \n",
            "\n",
            "\n",
            "Test Loss: 0.400 | | Test Acc: 83.51%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy of the Models')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVcvyLjUgIU"
      },
      "source": [
        "Deeper network has increased accuracy a little bit however training time has increased too much "
      ]
    }
  ]
}